{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用tf模拟观察梯度下降的过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 0 epoch,w is 2.600000,loss is 36.000000\n",
      "After 1 epoch,w is 1.160000,loss is 12.959999\n",
      "After 2 epoch,w is 0.296000,loss is 4.665599\n",
      "After 3 epoch,w is -0.222400,loss is 1.679616\n",
      "After 4 epoch,w is -0.533440,loss is 0.604662\n",
      "After 5 epoch,w is -0.720064,loss is 0.217678\n",
      "After 6 epoch,w is -0.832038,loss is 0.078364\n",
      "After 7 epoch,w is -0.899223,loss is 0.028211\n",
      "After 8 epoch,w is -0.939534,loss is 0.010156\n",
      "After 9 epoch,w is -0.963720,loss is 0.003656\n",
      "After 10 epoch,w is -0.978232,loss is 0.001316\n",
      "After 11 epoch,w is -0.986939,loss is 0.000474\n",
      "After 12 epoch,w is -0.992164,loss is 0.000171\n",
      "After 13 epoch,w is -0.995298,loss is 0.000061\n",
      "After 14 epoch,w is -0.997179,loss is 0.000022\n",
      "After 15 epoch,w is -0.998307,loss is 0.000008\n",
      "After 16 epoch,w is -0.998984,loss is 0.000003\n",
      "After 17 epoch,w is -0.999391,loss is 0.000001\n",
      "After 18 epoch,w is -0.999634,loss is 0.000000\n",
      "After 19 epoch,w is -0.999781,loss is 0.000000\n",
      "After 20 epoch,w is -0.999868,loss is 0.000000\n",
      "After 21 epoch,w is -0.999921,loss is 0.000000\n",
      "After 22 epoch,w is -0.999953,loss is 0.000000\n",
      "After 23 epoch,w is -0.999972,loss is 0.000000\n",
      "After 24 epoch,w is -0.999983,loss is 0.000000\n",
      "After 25 epoch,w is -0.999990,loss is 0.000000\n",
      "After 26 epoch,w is -0.999994,loss is 0.000000\n",
      "After 27 epoch,w is -0.999996,loss is 0.000000\n",
      "After 28 epoch,w is -0.999998,loss is 0.000000\n",
      "After 29 epoch,w is -0.999999,loss is 0.000000\n",
      "After 30 epoch,w is -0.999999,loss is 0.000000\n",
      "After 31 epoch,w is -1.000000,loss is 0.000000\n",
      "After 32 epoch,w is -1.000000,loss is 0.000000\n",
      "After 33 epoch,w is -1.000000,loss is 0.000000\n",
      "After 34 epoch,w is -1.000000,loss is 0.000000\n",
      "After 35 epoch,w is -1.000000,loss is 0.000000\n",
      "After 36 epoch,w is -1.000000,loss is 0.000000\n",
      "After 37 epoch,w is -1.000000,loss is 0.000000\n",
      "After 38 epoch,w is -1.000000,loss is 0.000000\n",
      "After 39 epoch,w is -1.000000,loss is 0.000000\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "w = tf.Variable(tf.constant(5, dtype=tf.float32))\n",
    "lr = 0.2\n",
    "epoch = 40\n",
    "\n",
    "for epoch in range(epoch):  # for epoch 定义顶层循环，表示对数据集循环epoch次，此例数据集数据仅有1个w,初始化时候constant赋值为5，循环40次迭代。\n",
    "    with tf.GradientTape() as tape:  # with结构到grads框起了梯度的计算过程。\n",
    "        loss = tf.square(w + 1)\n",
    "    grads = tape.gradient(loss, w)  # .gradient函数告知谁对谁求导\n",
    "\n",
    "    w.assign_sub(lr * grads)  # .assign_sub 对变量做自减 即：w -= lr*grads 即 w = w - lr*grads\n",
    "    print(\"After %s epoch,w is %f,loss is %f\" % (epoch, w.numpy(), loss))\n",
    "\n",
    "# lr初始值：0.2   请自改学习率  0.001  0.999 看收敛过程\n",
    "# 最终目的：找到 loss 最小 即 w = -1 的最优参数w\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建一个简单的TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(1.0, shape=(), dtype=float32)\n",
      "a = 1.0\n",
      "a.dtype = <dtype: 'float32'>\n",
      "a.shape = ()\n",
      "=============================================\n",
      "tf.Tensor([1. 2.], shape=(2,), dtype=float32)\n",
      "b = [1. 2.]\n",
      "b.dtype = <dtype: 'float32'>\n",
      "b.shape = (2,)\n",
      "=============================================\n",
      "tf.Tensor(\n",
      "[[1. 1.]\n",
      " [2. 2.]\n",
      " [3. 3.]], shape=(3, 2), dtype=float32)\n",
      "c = [[1. 1.]\n",
      " [2. 2.]\n",
      " [3. 3.]]\n",
      "c.dtype = <dtype: 'float32'>\n",
      "c.shape = (3, 2)\n"
     ]
    }
   ],
   "source": [
    "# 创建一个TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "a = tf.constant(1, dtype=tf.float32)\n",
    "b = tf.constant([1,2], dtype=tf.float32)\n",
    "c = tf.constant([[1,1],[2,2],[3,3]], dtype=tf.float32)\n",
    "\n",
    "print(a)\n",
    "print(f'a = {a}')\n",
    "print(f'a.dtype = {a.dtype}')\n",
    "print(f'a.shape = {a.shape}')\n",
    "print('=============================================')\n",
    "\n",
    "print(b)\n",
    "print(f'b = {b}')\n",
    "print(f'b.dtype = {b.dtype}')\n",
    "print(f'b.shape = {b.shape}')\n",
    "print('=============================================')\n",
    "\n",
    "print(c)\n",
    "print(f'c = {c}')\n",
    "print(f'c.dtype = {c.dtype}')\n",
    "print(f'c.shape = {c.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 将numpy的数据类型转换为tensor数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4]\n",
      "tf.Tensor([0. 1. 2. 3. 4.], shape=(5,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 将numpy的数据类型转换为tensor数据类型\n",
    "import numpy as np\n",
    "\n",
    "a = np.arange(0,5)\n",
    "print(a)\n",
    "b = tf.convert_to_tensor(a,dtype=tf.float32)\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用TensorFlow创建不同纬度的的数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------fill-------------------------------\n",
      "tf.Tensor([9], shape=(1,), dtype=int32)\n",
      "=================================================\n",
      "tf.Tensor(\n",
      "[[8 8]\n",
      " [8 8]], shape=(2, 2), dtype=int32)\n",
      "=================================================\n",
      "tf.Tensor(\n",
      "[[[7 7 7]\n",
      "  [7 7 7]\n",
      "  [7 7 7]]\n",
      "\n",
      " [[7 7 7]\n",
      "  [7 7 7]\n",
      "  [7 7 7]]\n",
      "\n",
      " [[7 7 7]\n",
      "  [7 7 7]\n",
      "  [7 7 7]]], shape=(3, 3, 3), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 使用TensorFlow创建不同纬度的的数组\n",
    "\n",
    "# param 数组的纬度  值全部为0\n",
    "# 纬度\n",
    "# 1维： 直接写个数\n",
    "# 2维： 用(行，列)  输出[行，列]\n",
    "# 3维： 用(行，列，层) 输出[行，列，层]\n",
    "# n维： 用(行，列，层，...) 输出[行，列，层...]\n",
    "a1 = tf.zeros(1)\n",
    "a2 = tf.zeros((2,2))\n",
    "a3 = tf.zeros((3,3,3))\n",
    "# print('------------zeros-----------------------------')\n",
    "# print(a1)\n",
    "# print('=================================================')\n",
    "# print(a2)\n",
    "# print('=================================================')\n",
    "# print(a3)\n",
    "\n",
    "# print('-------------ones--------------------------------')\n",
    "# # param 数组的纬度  值全部为1\n",
    "# b1 = tf.ones(1)\n",
    "# b2 = tf.ones((2,2))\n",
    "# print(b1)\n",
    "# print('=================================================')\n",
    "# print(b2)\n",
    "\n",
    "print('----------------fill-------------------------------')\n",
    "# 创建全为指定值的数组\n",
    "c1 = tf.fill(1,9)\n",
    "c2 = tf.fill((2,2),8)\n",
    "c3 = tf.fill((3,3,3),7)\n",
    "print(c1)\n",
    "print('=================================================')\n",
    "print(c2)\n",
    "print('=================================================')\n",
    "print(c3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用TensorFlow 生成正太分布的随机数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1.6755232 1.3096004 2.626256 ]\n",
      " [3.099818  1.5037482 1.2535437]], shape=(2, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[2.3492804  0.30397093 1.5604835 ]\n",
      " [1.3046471  1.1749543  1.5334079 ]\n",
      " [0.14099407 0.12230831 2.4702039 ]], shape=(3, 3), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[-1.1929271  -0.5396427  -0.51999027]\n",
      "  [ 0.03832126 -0.9764158   2.1505182 ]\n",
      "  [-0.20639803  0.00585017  0.69992405]]\n",
      "\n",
      " [[ 1.8584644  -0.9749002   1.1374246 ]\n",
      "  [ 0.2115156  -0.63308686 -1.3756505 ]\n",
      "  [ 0.7488925  -0.6113612   0.9755997 ]]\n",
      "\n",
      " [[ 1.2926517  -0.5451891   0.29370302]\n",
      "  [-0.38894567  0.4151528  -0.44211608]\n",
      "  [-0.4689476  -1.4655814  -0.5997934 ]]], shape=(3, 3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 使用TensorFlow 生成正太分布的随机数 所有值都可能出现。\n",
    "# parameters: shape = 形状, mean = 均值, stddev = 标准差 结果都在俩倍标准差内 ，数据都在均值附近。\n",
    "a = tf.random.normal(shape=(2, 3), mean=1.0, stddev=1.0)\n",
    "print(a)\n",
    "\n",
    "# 使用TensorFlow 生成截断正太分布的随机数 舍弃了极端值，更平滑。\n",
    "# parameters: shape = 形状, mean = 均值, stddev = 标准差\n",
    "b = tf.random.truncated_normal(shape=(3, 3), mean=1.0, stddev=1.0)\n",
    "print(b)\n",
    "\n",
    "c = tf.random.normal(shape= (3,3,3), mean=0.0, stddev=1.0)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用tf生成均匀分布的随机数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0.8786297  0.40170407]\n",
      " [0.4544171  0.03021514]], shape=(2, 2), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[[6.0782564e-01 6.6189969e-01 6.4920962e-01]\n",
      "  [1.2656271e-01 4.3745995e-01 5.0298190e-01]\n",
      "  [1.9666672e-01 7.8926504e-01 8.4545839e-01]]\n",
      "\n",
      " [[1.9708371e-01 4.2974949e-04 9.9448299e-01]\n",
      "  [8.0319953e-01 5.4251420e-01 4.2206776e-01]\n",
      "  [8.4026217e-01 2.5527239e-02 5.9680498e-01]]\n",
      "\n",
      " [[7.5571537e-03 2.4450099e-01 2.3040354e-01]\n",
      "  [2.6115513e-01 7.6110709e-01 9.8826993e-01]\n",
      "  [2.2702920e-01 2.7402782e-01 5.2210724e-01]]], shape=(3, 3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 生成均匀分布的随机数 [max,min)\n",
    "a = tf.random.uniform(shape=(2,2),minval=0,maxval=1)\n",
    "print(a)\n",
    "\n",
    "b = tf.random.uniform(shape=(3,3,3),minval=0,maxval=1)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 强制TensorFlow 转换为该数据类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1. 2. 3.], shape=(3,), dtype=float32)\n",
      "tf.Tensor([1 2 3], shape=(3,), dtype=int32)\n",
      "tf.Tensor(3, shape=(), dtype=int32)\n",
      "tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# 强制TensorFlow 转换为该数据类型\n",
    "\n",
    "# define  constant tensor\n",
    "a = tf.constant([1.0, 2.0,3.0], dtype=tf.float32)\n",
    "print(a)\n",
    "\n",
    "# cast the tensor to another data type\n",
    "b = tf.cast(a, tf.int32)\n",
    "print(b)\n",
    "\n",
    "# get the max and min value of tensor\n",
    "print(tf.reduce_max(b))\n",
    "print(tf.reduce_min(b))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 理解axis参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a is : [[1 1]\n",
      " [1 1]]\n",
      "sum_all is : 4\n",
      "sum_down_all is : [2 2]\n",
      "sum_across_all is : [2 2]\n",
      "mean_all is : 1\n",
      "mean_down_all is : [1 1]\n",
      "mean_across_all is : [1 1]\n"
     ]
    }
   ],
   "source": [
    "# 理解axis参数\n",
    "# axis = 0，沿着列进行操作 （经度，跨行，dwon）\n",
    "# axis = 1，沿着行进行操作 （纬度，跨列，across）\n",
    "# 不指定axis ，确认对整个矩阵进行操作\n",
    "\n",
    "a = tf.fill((2,2),1)\n",
    "print(f'a is : {a}')\n",
    "\n",
    "# 求和\n",
    "sum_all = tf.reduce_sum(a)\n",
    "print(f'sum_all is : {sum_all}')\n",
    "\n",
    "sum_down_all = tf.reduce_sum(a,axis=0)\n",
    "print(f'sum_down_all is : {sum_down_all}')\n",
    "\n",
    "sum_across_all = tf.reduce_sum(a,axis=1)\n",
    "print(f'sum_across_all is : {sum_across_all}')\n",
    "\n",
    "\n",
    "# 求平均值\n",
    "mean_all = tf.reduce_mean(a)\n",
    "print(f'mean_all is : {mean_all}')\n",
    "\n",
    "mean_down_all = tf.reduce_mean(a,axis=0)\n",
    "print(f'mean_down_all is : {mean_down_all}')\n",
    "\n",
    "mean_across_all = tf.reduce_mean(a,axis=1)\n",
    "print(f'mean_across_all is : {mean_across_all}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w: <tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=\n",
      "array([[ 1.73464   ,  0.935121  , -1.7392381 ],\n",
      "       [-0.44500777,  1.32529   ,  0.39678475],\n",
      "       [ 1.241507  , -0.7415922 , -1.8283793 ]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "# tf.Variable 函数将变量标记为可训练，被标记的变量会在反向传播中计算梯度\n",
    "# 在神经网络中，常用该函数来定义权重和偏置\n",
    "\n",
    "w = tf.Variable(tf.random.normal([3, 3], mean = 0, stddev = 1))\n",
    "print(f'w: {w}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow中的数学计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "a = [[1. 1.]]\n",
      "\n",
      "b = [[2. 2.]]\n",
      "\n",
      "a + b = [[3. 3.]]\n",
      "\n",
      "a * b = [[2. 2.]]\n",
      "\n",
      "a / b = [[0.5 0.5]]\n",
      "\n",
      "a - b = [[-1. -1.]]\n",
      "\n",
      "c = [[4. 4.]]\n",
      "\n",
      "c ** 3 = [[64. 64.]]\n",
      "\n",
      "d = [[2. 2.]]\n",
      "\n",
      "e1 = [[1. 1.]\n",
      " [1. 1.]\n",
      " [1. 1.]]\n",
      "\n",
      "e2 = [[3. 3. 3.]\n",
      " [3. 3. 3.]]\n",
      "\n",
      "e = [[6. 6. 6.]\n",
      " [6. 6. 6.]\n",
      " [6. 6. 6.]]\n"
     ]
    }
   ],
   "source": [
    "# 四则运算 add subtract multiply divide\n",
    "# 平方、次方、开方 square power sqrt\n",
    "# 矩阵乘 muamul\n",
    "\n",
    "# 只有纬度相等的矩阵才能进行乘法运算\n",
    "# 注意：tensorflow 四则运算时，俩个矩阵的数据类型要相同\n",
    "# 四则运算时，如果形状不同，会通过广播机制调整形状\n",
    "\n",
    "import tensorflow as tf\n",
    "a = tf.ones([1,2])\n",
    "b = tf.fill([1,2], 2.)\n",
    "print(f'\\na = {a}')\n",
    "print(f'\\nb = {b}')\n",
    "\n",
    "print(f'\\na + b = {tf.add(a,b)}')\n",
    "print(f'\\na * b = {tf.multiply(a,b)}')\n",
    "print(f'\\na / b = {tf.divide(a,b)}')\n",
    "print(f'\\na - b = {tf.subtract(a,b)}')\n",
    "\n",
    "# 平方\n",
    "c = tf.square(b)\n",
    "print(f'\\nc = {c}')\n",
    "\n",
    "# 次方 此时为3次方\n",
    "print(f'\\nc ** 3 = {tf.pow(c,3)}')\n",
    "\n",
    "# 开方\n",
    "d = tf.sqrt(c)\n",
    "print(f'\\nd = {d}')\n",
    "\n",
    "\n",
    "# 矩阵乘\n",
    "# 注意：tensorflow 在进行矩阵乘时，要求两个矩阵的纬度必须满足条件：\n",
    "# 1. 第一个矩阵的列数 = 第二个矩阵的行数\n",
    "# 2. 两个矩阵的数据类型必须相同\n",
    "e1 = tf.ones([3,2])\n",
    "e2 = tf.fill([2,3], 3.)\n",
    "e = tf.matmul(e1,e2)\n",
    "print(f'\\ne1 = {e1}')\n",
    "print(f'\\ne2 = {e2}')\n",
    "print(f'\\ne = {e}')\n",
    "# e3 = tf.matmul(e2,e1)\n",
    "# print(f'\\ne3 = {e3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multiply 和 matmul 有什么区别？\n",
    "\n",
    "| 特性           | `tf.multiply`                        | `tf.matmul`                               |\n",
    "|----------------|--------------------------------------|-------------------------------------------|\n",
    "| 运算类型        | 逐元素相乘                          | 矩阵乘法                                  |\n",
    "| 广播支持        | 支持                                 | 不支持（但支持高维批量矩阵乘法）          |\n",
    "| 输入形状要求    | 形状相同或可广播                    | 矩阵乘法规则（前者的列数等于后者的行数）  |\n",
    "| 常用场景        | 逐元素计算，点积                    | 线性代数运算，深度学习模型的权重操作      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 切片传入张量的第一维度，生成输入特征/标签 构建数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(), dtype=int32, numpy=1>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=2>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=3>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=4>, <tf.Tensor: shape=(), dtype=int32, numpy=1>)\n",
      "(<tf.Tensor: shape=(), dtype=int32, numpy=5>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-28 17:15:38.888271: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int32 and shape [5]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    }
   ],
   "source": [
    "# 切片传入张量的第一维度，生成输入特征/标签 构建数据集\n",
    "# data = tf.data.Dataset.from_tensor_slices((x, y))\n",
    "\n",
    "feauture = tf.constant([1,2,3,4,5])\n",
    "label = tf.constant([0,1,0,1,0])\n",
    "result = tf.data.Dataset.from_tensor_slices((feauture, label))\n",
    "for i in result:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用GradientTape 实现某个函数对指定的变量求导"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(6.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# tf.GradientTape()  在with结构中，使用tf.GradientTape()实现某个函数对制定参数的求导运算\n",
    "# with tf.GradientTape() as tape:\n",
    "# grad = tape.gradient(y, x)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    # 定义一个变量 w，初始值为常量 3.0。\n",
    "\t# 使用 tf.Variable 表明 w 是一个可训练变量（即可以对它求导和更新）。\n",
    "    w = tf.Variable(tf.constant(3.0))\n",
    "    # 定义损失函数 loss，这里是  w^2 \n",
    "    loss = tf.pow(w, 2)\n",
    "# 用 tape.gradient(target, sources) 计算目标值 loss 对源变量 w 的梯度\n",
    "grad = tape.gradient(loss, w)\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "进入上下文\n",
      "使用 资源\n",
      "退出上下文\n"
     ]
    }
   ],
   "source": [
    "class MyContext:\n",
    "    def __enter__(self):\n",
    "        print(\"进入上下文\")\n",
    "        return \"资源\"\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        print(\"退出上下文\")\n",
    "        return True  # 如果返回 True，抑制异常传播\n",
    "\n",
    "with MyContext() as resource:\n",
    "    print(\"使用\", resource)\n",
    "    raise ValueError(\"抛出异常\")\n",
    "# 输出:\n",
    "# 进入上下文\n",
    "# 使用 资源\n",
    "# 退出上下文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 a\n",
      "1 b\n",
      "2 c\n"
     ]
    }
   ],
   "source": [
    "# enumerate 函数 用于将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标，一般用在 for 循环当中。\n",
    "array = ['a', 'b', 'c']\n",
    "for i, value in enumerate(array):\n",
    "    print(i, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.one_hot 独热编码 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]], shape=(3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# tf.one_hot 独热编码 在分类问题中，我们通常会使用独热编码来表示一个类别。\n",
    "# 将待转换数据 转换为 one_hot形式的数据输出\n",
    "# tf.one_hot(data, depth= 几分类)\n",
    "classes =3\n",
    "labels = tf.constant([1, 2, 0])\n",
    "one_hot = tf.one_hot(labels, classes)\n",
    "print(one_hot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用tf.nn.softmax(x) 使结果符合概率分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.08134112 0.24726778 0.67139107], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 使用tf.nn.softmax(x) 使结果符合概率分布\n",
    "# 当n分类的n个输出（y0，y1，y2，y3，y4） 通过tf.nn.softmax(x)函数后符合概率分布 y0+y1+y2+y3+y4=1\n",
    "\n",
    "y = tf.constant([1.2213, 2.33312, 3.332])\n",
    "y_pro = tf.nn.softmax(y)\n",
    "print(y_pro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用assign_sub 函数更新参数的值并返回"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'weight:0' shape=() dtype=int32, numpy=9>\n"
     ]
    }
   ],
   "source": [
    "# 使用assign_sub 函数更新参数的值并返回\n",
    "# 调用assign_sub时，需要先用tf.Variable()函数创建变量，定义变量为可训练的\n",
    "\n",
    "w = tf.Variable(10, name='weight')\n",
    "w.assign_sub(1)\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用tf.argmax()函数返回张量沿指定轴的最大值的索引"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]]\n",
      "tf.Tensor([2 2 2 2], shape=(4,), dtype=int64)\n",
      "tf.Tensor([3 3 3], shape=(3,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "array = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12]])\n",
    "print(array)\n",
    "# 返回每一列（经度）最大值的索引\n",
    "print(tf.argmax(array,axis=0))\n",
    "# 返回每一行（纬度）最大值的索引\n",
    "print(tf.argmax(array,axis=1))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
