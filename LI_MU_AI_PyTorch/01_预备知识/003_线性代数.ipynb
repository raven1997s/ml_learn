{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caf050db",
   "metadata": {},
   "source": [
    "# 线性代数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "368dd6d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([3.]), y: tensor([4.])\n",
      "x + y: tensor([7.])\n",
      "x - y: tensor([-1.])\n",
      "x * y: tensor([12.])\n",
      "x / y: tensor([0.7500])\n"
     ]
    }
   ],
   "source": [
    "# 标量\n",
    "import torch\n",
    "x = torch.tensor([3.0])\n",
    "y = torch.tensor([4.0])\n",
    "print(f\"x: {x}, y: {y}\")\n",
    "print(f\"x + y: {x + y}\")\n",
    "print(f\"x - y: {x - y}\")\n",
    "print(f\"x * y: {x * y}\")\n",
    "print(f\"x / y: {x / y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57c1436b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([0, 1, 2, 3])\n",
      "x shape: torch.Size([4])\n",
      "x len : 4\n",
      "x[2]: 2\n",
      "x[2] shape: torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "# 向量\n",
    "x = torch.arange(4)\n",
    "print(f\"x: {x}\")\n",
    "print(f\"x shape: {x.shape}\")\n",
    "print(f\"x len : {len(x)}\")\n",
    "\n",
    "x_2 = x[2]\n",
    "print(f\"x[2]: {x_2}\")\n",
    "print(f\"x[2] shape: {x_2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "066e3acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "x.T: tensor([[ 0,  4,  8],\n",
      "        [ 1,  5,  9],\n",
      "        [ 2,  6, 10],\n",
      "        [ 3,  7, 11]])\n",
      "B: tensor([[1, 2, 3],\n",
      "        [2, 0, 4],\n",
      "        [3, 4, 5]])\n",
      "B.T: tensor([[1, 2, 3],\n",
      "        [2, 0, 4],\n",
      "        [3, 4, 5]])\n",
      "B == B.T: tensor([[True, True, True],\n",
      "        [True, True, True],\n",
      "        [True, True, True]])\n",
      "A: tensor([[[ 0,  1,  2,  3],\n",
      "         [ 4,  5,  6,  7],\n",
      "         [ 8,  9, 10, 11]],\n",
      "\n",
      "        [[12, 13, 14, 15],\n",
      "         [16, 17, 18, 19],\n",
      "         [20, 21, 22, 23]]])\n",
      "A shape: torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "# 矩阵\n",
    "x = torch.arange(12).reshape(3, 4)\n",
    "print(f\"x: {x}\")\n",
    "\n",
    "# 矩阵转置\n",
    "print(f\"x.T: {x.T}\")\n",
    "\n",
    "# 对称矩阵,除对角线外的元素对称\n",
    "B = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n",
    "print(f\"B: {B}\")\n",
    "print(f\"B.T: {B.T}\")\n",
    "print(f\"B == B.T: {B == B.T}\")\n",
    "\n",
    "# 多维矩阵\n",
    "A = torch.arange(24).reshape(2, 3, 4)\n",
    "print(f\"A: {A}\")\n",
    "print(f\"A shape: {A.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b715d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: tensor([[ 0,  1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14],\n",
      "        [15, 16, 17, 18, 19]])\n",
      "B: tensor([[ 0,  1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14],\n",
      "        [15, 16, 17, 18, 19]])\n",
      "A + B: tensor([[ 0,  2,  4,  6,  8],\n",
      "        [10, 12, 14, 16, 18],\n",
      "        [20, 22, 24, 26, 28],\n",
      "        [30, 32, 34, 36, 38]])\n",
      "A * B: tensor([[  0,   1,   4,   9,  16],\n",
      "        [ 25,  36,  49,  64,  81],\n",
      "        [100, 121, 144, 169, 196],\n",
      "        [225, 256, 289, 324, 361]])\n",
      "A + a: tensor([[ 1,  2,  3,  4,  5],\n",
      "        [ 6,  7,  8,  9, 10],\n",
      "        [11, 12, 13, 14, 15],\n",
      "        [16, 17, 18, 19, 20]])\n",
      "X: tensor([0, 1, 2, 3])\n",
      "X.sum(): 6\n",
      "A shape: torch.Size([4, 5])\n",
      "A sum: 190\n",
      "A.sum(axis=0): tensor([30, 34, 38, 42, 46])\n",
      "A.sum(axis=1): tensor([10, 35, 60, 85])\n",
      "A.mean(): 9.5\n",
      "A numels: 20\n",
      "A.sum()/A.numel(): 9.5\n",
      "sum_A: tensor([[30., 34., 38., 42., 46.]])\n",
      "sumA shape: torch.Size([1, 5])\n",
      "A/sum_A: tensor([[0.0000, 0.0294, 0.0526, 0.0714, 0.0870],\n",
      "        [0.1667, 0.1765, 0.1842, 0.1905, 0.1957],\n",
      "        [0.3333, 0.3235, 0.3158, 0.3095, 0.3043],\n",
      "        [0.5000, 0.4706, 0.4474, 0.4286, 0.4130]])\n",
      "A.cumsum(axis=0): tensor([[ 0.,  1.,  2.,  3.,  4.],\n",
      "        [ 5.,  7.,  9., 11., 13.],\n",
      "        [15., 18., 21., 24., 27.],\n",
      "        [30., 34., 38., 42., 46.]])\n",
      "A.cumsum(axis=1): tensor([[ 0.,  1.,  3.,  6., 10.],\n",
      "        [ 5., 11., 18., 26., 35.],\n",
      "        [10., 21., 33., 46., 60.],\n",
      "        [15., 31., 48., 66., 85.]])\n"
     ]
    }
   ],
   "source": [
    "# 矩阵克隆\n",
    "A = torch.arange(20).reshape(4, 5)\n",
    "B = A.clone()\n",
    "print(f\"A: {A}\")\n",
    "print(f\"B: {B}\")\n",
    "print(f\"A + B: {A + B}\")\n",
    "\n",
    "# 矩阵相乘： 对应元素相乘\n",
    "print(f\"A * B: {A * B}\")\n",
    "\n",
    "# 矩阵+ 标量\n",
    "a = 1\n",
    "print(f\"A + a: {A + a}\")\n",
    "\n",
    "# 向量求和\n",
    "X = torch.arange(4)\n",
    "print(f\"X: {X}\")\n",
    "print(f\"X.sum(): {X.sum()}\")\n",
    "\n",
    "# 矩阵求和\n",
    "# print(f\"A: {A}\")\n",
    "print(f\"A shape: {A.shape}\")\n",
    "print(f\"A sum: {A.sum()}\")\n",
    "\n",
    "# 矩阵按列求和(维度丢失)\n",
    "print(f\"A.sum(axis=0): {A.sum(axis=0)}\")\n",
    "# 矩阵按行求和\n",
    "print(f\"A.sum(axis=1): {A.sum(axis=1)}\")\n",
    "\n",
    "# 矩阵求均值\n",
    "A = torch.arange(20, dtype=torch.float32).reshape(4, 5)\n",
    "print(f\"A.mean(): {A.mean()}\")\n",
    "print(f\"A numels: {A.numel()}\")\n",
    "print(f\"A.sum()/A.numel(): {A.sum()/A.numel()}\")\n",
    "\n",
    "# 矩阵按行求和(纬度不丢失)\n",
    "sum_A = A.sum(axis=0, dtype=torch.float32, keepdim=True)\n",
    "print(f\"sum_A: {sum_A}\")\n",
    "print(f\"sumA shape: {sum_A.shape}\")\n",
    "\n",
    "# 矩阵广播\n",
    "print(f\"A/sum_A: {A/sum_A}\")\n",
    "\n",
    "# 矩阵延某轴累加 0 :  行 1 : 列\n",
    "print(f\"A.cumsum(axis=0): {A.cumsum(axis=0)}\")\n",
    "print(f\"A.cumsum(axis=1): {A.cumsum(axis=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0a1fd8e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: tensor([0., 1., 2., 3.])\n",
      "y: tensor([1., 1., 1., 1.])\n",
      "x dot y: 6.0\n",
      "torch.sum(x * y): 6.0\n",
      "A: tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n",
      "x: tensor([0., 1., 2., 3.])\n",
      "A shape: torch.Size([5, 4])\n",
      "x shape: torch.Size([4])\n",
      "A * x: tensor([ 14.,  38.,  62.,  86., 110.])\n",
      "A: tensor([[ 0.,  1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.,  7.],\n",
      "        [ 8.,  9., 10., 11.],\n",
      "        [12., 13., 14., 15.],\n",
      "        [16., 17., 18., 19.]])\n",
      "B: tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "A * B: tensor([[ 6.,  6.,  6.],\n",
      "        [22., 22., 22.],\n",
      "        [38., 38., 38.],\n",
      "        [54., 54., 54.],\n",
      "        [70., 70., 70.]])\n"
     ]
    }
   ],
   "source": [
    "# 向量点积\n",
    "x = torch.arange(4, dtype=torch.float32)\n",
    "y = torch.ones(4, dtype=torch.float32)\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y: {y}\")\n",
    "print(f\"x dot y: {torch.dot(x, y)}\")\n",
    "\n",
    "print(f\"torch.sum(x * y): {torch.sum(x * y)}\")\n",
    "\n",
    "# 矩阵向量积\n",
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "x = torch.arange(4, dtype=torch.float32)\n",
    "print(f\"A: {A}\")\n",
    "print(f\"x: {x}\")\n",
    "print(f\"A shape: {A.shape}\")\n",
    "print(f\"x shape: {x.shape}\")\n",
    "print(f\"A * x: {torch.mv(A, x)}\")\n",
    "\n",
    "# 矩阵相乘\n",
    "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
    "B = torch.ones((4, 3), dtype=torch.float32)\n",
    "print(f\"A: {A}\")\n",
    "print(f\"B: {B}\")\n",
    "print(f\"A * B: {torch.mm(A, B)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "34ac6fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.norm(u) = 5.0\n",
      "torch.abs(u).sum(): 7.0\n"
     ]
    }
   ],
   "source": [
    "# 矩阵L2 范数： 向量元素平方和的平方根\n",
    "u = torch.tensor([3.0, -4.0])\n",
    "print(f\"torch.norm(u) = {torch.norm(u)}\")\n",
    "\n",
    "# 矩阵L1 范数：  向量元素绝对值的和\n",
    "print(f\"torch.abs(u).sum(): {torch.abs(u).sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f51c0597",
   "metadata": {},
   "source": [
    "# 练习\n",
    "### 1. 证明一个矩阵的转置的转置是其本身，即 $(A^\\top)^\\top = A$。\n",
    "\n",
    "**证明：**\n",
    "设矩阵 $A$ 的元素为 $a_{ij}$，其中 $i$ 表示行索引，$j$ 表示列索引。根据转置的定义：\n",
    "- $A^\\top$ 的元素为 $a_{ji}$（即行和列交换）。\n",
    "- $(A^\\top)^\\top$ 的元素为 $a_{ij}$（再次交换行和列）。\n",
    "\n",
    "因此，$(A^\\top)^\\top$ 的元素与 $A$ 的元素完全相同，即 $(A^\\top)^\\top = A$。\n",
    "\n",
    "### 2. 证明“两个矩阵转置的和”等于“它们和的转置”，即 $(A + B)^\\top = A^\\top + B^\\top$。\n",
    "\n",
    "**证明：**\n",
    "设矩阵 $A$ 和 $B$ 的元素分别为 $a_{ij}$ 和 $b_{ij}$。\n",
    "- $A + B$ 的元素为 $a_{ij} + b_{ij}$。\n",
    "- $(A + B)^\\top$ 的元素为 $a_{ji} + b_{ji}$。\n",
    "- $A^\\top$ 的元素为 $a_{ji}$，$B^\\top$ 的元素为 $b_{ji}$。\n",
    "- $A^\\top + B^\\top$ 的元素为 $a_{ji} + b_{ji}$。\n",
    "\n",
    "因此，$(A + B)^\\top$ 和 $A^\\top + B^\\top$ 的元素完全相同，即 $(A + B)^\\top = A^\\top + B^\\top$。\n",
    "\n",
    "### 3. 给定任意方阵 $A$，$A + A^\\top$ 总是对称的吗？为什么？\n",
    "\n",
    "**回答：**\n",
    "是的，$A + A^\\top$ 总是对称的。\n",
    "\n",
    "**证明：**\n",
    "对称矩阵的定义是 $M = M^\\top$。我们验证 $(A + A^\\top)^\\top$：\n",
    "- $(A + A^\\top)^\\top = A^\\top + (A^\\top)^\\top = A^\\top + A = A + A^\\top$。\n",
    "因此，$A + A^\\top$ 是对称的。\n",
    "\n",
    "### 4. 对于形状为 $(2, 3, 4)$ 的张量 $X$，`len(X)` 的输出结果是什么？\n",
    "\n",
    "**回答：**\n",
    "在 Python 中，`len(X)` 返回张量 $X$ 的第一个轴（轴 0）的长度。对于形状为 $(2, 3, 4)$ 的张量，`len(X)` 的输出是 $2$。\n",
    "\n",
    "### 5. 对于任意形状的张量 $X$，`len(X)` 是否总是对应于 $X$ 特定轴的长度？这个轴是什么？\n",
    "\n",
    "**回答：**\n",
    "是的，`len(X)` 总是返回张量 $X$ 的第一个轴（轴 0）的长度。例如：\n",
    "- 形状为 $(a, b, c)$ 的张量，`len(X)` 返回 $a$。\n",
    "- 形状为 $(a,)$ 的张量，`len(X)` 返回 $a$。\n",
    "- 标量（形状为 $()$）会报错，因为标量没有长度。\n",
    "\n",
    "### 6. 运行 `A / A.sum(axis=1)`，看看会发生什么？请分析原因。\n",
    "\n",
    "**分析：**\n",
    "假设 $A$ 是一个矩阵（二维张量），`A.sum(axis=1)` 会对 $A$ 的每一行求和，得到一个形状为 $(n,)$ 或 $(n, 1)$ 的向量（取决于广播规则）。然后 `A / A.sum(axis=1)` 会将 $A$ 的每一行除以其行和。\n",
    "\n",
    "**可能的问题：**\n",
    "- 如果 `A.sum(axis=1)` 的形状是 $(n,)` 而 $A$ 的形状是 $(n, m)$，需要广播。通常 `A.sum(axis=1)` 会扩展为 $(n, 1)$ 以匹配 $A$ 的列。\n",
    "- 如果 `A.sum(axis=1)` 中有零元素，会导致除以零的错误。\n",
    "\n",
    "### 7. 考虑一个形状为 $(2, 3, 4)$ 的张量，在轴 0、1、2 上的求和输出形状是什么？\n",
    "\n",
    "**回答：**\n",
    "- 沿轴 0 求和：形状为 $(3, 4)$（去掉轴 0）。\n",
    "- 沿轴 1 求和：形状为 $(2, 4)$（去掉轴 1）。\n",
    "- 沿轴 2 求和：形状为 $(2, 3)$（去掉轴 2）。\n",
    "\n",
    "### 8. 为 `linalg.norm` 函数提供 3 个或更多轴的张量，并观察其输出。对于任意形状的张量，这个函数计算得到什么？\n",
    "\n",
    "**回答：**\n",
    "`linalg.norm`（默认情况下）计算张量的 Frobenius 范数（即所有元素的平方和的平方根）。对于任意形状的张量：\n",
    "- 输出是一个标量（形状为 $()$），表示整个张量的 Frobenius 范数。\n",
    "- 例如，对于形状为 $(2, 3, 4)$ 的张量，`np.linalg.norm(X)` 返回 $\\sqrt{\\sum_{i=1}^2 \\sum_{j=1}^3 \\sum_{k=1}^4 X_{ijk}^2}$。\n",
    "\n",
    "可以通过 `axis` 参数指定沿特定轴计算范数，此时输出形状会去掉对应的轴。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7575fdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 6 15]\n",
      "[5 7 9]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = np.array([\n",
    "    [1, 2, 3],  # 第0行\n",
    "    [4, 5, 6]   # 第1行\n",
    "])\n",
    "\n",
    "#  行求和\n",
    "A_sum_row = np.sum(A, axis=1)\n",
    "print(A_sum_row)\n",
    "\n",
    "# 列求和\n",
    "A_sum_col = np.sum(A, axis=0)\n",
    "print(A_sum_col)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
