{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ece0620",
   "metadata": {},
   "source": [
    "# 线性回归(使用自定义函数)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1a4267",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import torch\n",
    "from d2l import torch as d2l\n",
    "\n",
    "# 1.生成合成数据集\n",
    "def synthetic_data(w,b,num_exaples):\n",
    "    \"\"\"生成 y = Xw + b + 噪声\"\"\"\n",
    "    X = torch.normal(0,1,(num_exaples,len(w)))\n",
    "    print(\"X.shape:\",X.shape)\n",
    "    y = torch.matmul(X,w) + b\n",
    "    print(\"y.shape:\",y.shape)\n",
    "    y += torch.normal(0,0.01,y.shape)\n",
    "    print(\"y.shape:\",y.shape)\n",
    "    return X, y.reshape((-1,1))\n",
    "\n",
    "true_w = torch.tensor([2,-3.4])\n",
    "true_b = 4.2\n",
    "features, labels = synthetic_data(true_w, true_b, 1000)\n",
    "print(\"features.shape:\",features.shape)\n",
    "print(\"labels.shape:\",labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef02fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2l.set_figsize()\n",
    "d2l.plt.scatter(features[:,1].detach().numpy(),labels.detach().numpy(),1)  # 只有detach后才能转到numpy里面去     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60e5df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据集迭代读取\n",
    "# data_iter函数接收批量大小、特征矩阵和标签向量作为输入，生成大小为batch_size的小批量\n",
    "def data_iter(batch_size,features,labels):\n",
    "    num_examples = len(features)  # 样本个数\n",
    "    indices = list(range(num_examples)) # 样本索引\n",
    "    # 这些样本是随即读取的，没有特定的顺序\n",
    "    random.shuffle(indices) # 把索引随即打乱\n",
    "    for i in range(0, num_examples, batch_size):\n",
    "        batch_indices = torch.tensor(indices[i:min(i+batch_size,num_examples)]) # 当i+batch_size超出时，取num_examples         \n",
    "        yield features[batch_indices], labels[batch_indices] # 获得随即顺序的特征，及对应的标签\n",
    "        \n",
    "batch_size = 10\n",
    "for X,y in data_iter(batch_size, features, labels):\n",
    "    print(X, '\\n', y) # 取一个批次后，就break跳出了\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189d30d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义初始化模型参数\n",
    "w = torch.normal(0,0.01,size=(2,1),requires_grad=True)\n",
    "b = torch.zeros(1,requires_grad=True)\n",
    "\n",
    "# 定义模型\n",
    "def linreg(X,w,b):\n",
    "    \"\"\"线性回归模型\"\"\"\n",
    "    return torch.matmul(X,w)+b\n",
    "\n",
    "# 定义损失函数\n",
    "def squared_loss(y_hat,y):\n",
    "    \"\"\"均方损失\"\"\"\n",
    "    return (y_hat - y.reshape(y_hat.shape))**2/2 # 将y统一成与y_hat一样同尺寸   \n",
    "\n",
    "# 定义优化算法\n",
    "def sgd(params,lr,batch_size):\n",
    "    \"\"\"小批量随即梯度下降\"\"\"\n",
    "    with torch.no_grad(): # 不要产生梯度计算，减少内存消耗\n",
    "        for param in params: # 每个参数进行遍历\n",
    "            param -= lr * param.grad / batch_size # 每个参数进行更新，损失函数没有求均值，所以这里除以 batch_size 求了均值。由于乘法的线性关系，这里除以放在loss的除以是等价的。                          \n",
    "            param.grad.zero_() # 每个参数的梯度清零\n",
    "\n",
    "# 训练过程\n",
    "lr = 0.03\n",
    "num_epochs = 3\n",
    "net = linreg # 这里用线性模型，这样写是很方便net赋予其他模型，只需要改一处，不需要下面所有网络模型名称都改\n",
    "loss = squared_loss\n",
    "\n",
    "# 训练过程\n",
    "for epoch in range(num_epochs):\n",
    "    for X,y in data_iter(batch_size,features,labels):\n",
    "        l = loss(net(X,w,b),y) # x和y的小批量损失\n",
    "        # 因为l是形状是(batch_size,1)，而不是一个标量。l中所有元素被加到一起\n",
    "        # 并以此计算关于[w,b]的梯度\n",
    "        l.sum().backward()\n",
    "        sgd([w,b],lr,batch_size) #使用参数的梯度更新参数\n",
    "    with torch.no_grad():\n",
    "        train_l = loss(net(features,w,b),labels)\n",
    "        print(f'epoch{epoch+1},loss{float(train_l.mean()):f}')   \n",
    "\n",
    "# 比较真实参数和通过训练学到的参数来评估训练的成功程度\n",
    "print(f'w的估计误差：{true_w-w.reshape(true_w.shape)}')\n",
    "print(f'b的估计误差：{true_b-b}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15861637",
   "metadata": {},
   "source": [
    "# 线性回归（使用PyTorch实现）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e8eb4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.000208\n",
      "epoch 2, loss 0.000098\n",
      "epoch 3, loss 0.000096\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from d2l import torch as d2l\n",
    "from torch import nn\n",
    "\n",
    "true_w = torch.tensor([2, -3.4])\n",
    "true_b = 4.2\n",
    "# 使用d2l.synthetic_data生成合成数据集\n",
    "features, labels = d2l.synthetic_data(true_w, true_b, 1000)\n",
    "\n",
    "#  使用框架现有API读取数据\n",
    "def load_array(data_arrays, batch_size, is_train=True):\n",
    "    \"\"\"构造一个PyTorch数据迭代器\"\"\"\n",
    "    dataset = data.TensorDataset(*data_arrays)  # 将特征和标签打包成一个数据集\n",
    "    return data.DataLoader(dataset, batch_size, shuffle=is_train)  # 返回数据迭代器\n",
    "\n",
    "batch_size = 10\n",
    "data_iter = load_array((features, labels), batch_size)\n",
    "# print(next(iter(data_iter)))  # 取出一个批次的数据 iter(data_iter)返回一个迭代器，next(iter(data_iter))取出一个批次的数据\n",
    "\n",
    "# 使用nn模块定义模型\n",
    "class LinearNet(nn.Module):\n",
    "    \"\"\"线性回归模型\"\"\"\n",
    "    def __init__(self):\n",
    "        super(LinearNet, self).__init__()\n",
    "        self.linear = nn.Linear(2, 1)  # 输入特征维度为2，输出维度为1\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.linear(X)\n",
    "    \n",
    "net = LinearNet()  # 实例化模型\n",
    "\n",
    "# 初始化模型参数\n",
    "net.linear.weight.data.normal_(0, 0.01) # 权重初始化为均值为0，标准差为0.01的正态分布\n",
    "net.linear.bias.data.fill_(0)  # 偏置初始化为0\n",
    "\n",
    "# 定义损失函数\n",
    "loss = nn.MSELoss()  # 均方误差损失函数\n",
    "# 定义优化算法\n",
    "def init_optimizer(net, lr):\n",
    "    \"\"\"初始化优化器\"\"\"\n",
    "    return torch.optim.SGD(net.parameters(), lr=lr)  # 使用随机梯度下降优化器\n",
    "\n",
    "lr = 0.03\n",
    "# 实例化优化器\n",
    "trainer = init_optimizer(net, lr)  # 初始化优化器\n",
    "\n",
    "num_epochs = 3\n",
    "# 训练过程\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in data_iter:\n",
    "        # 前向传播\n",
    "        l = loss(net(X), y)  # 计算损失\n",
    "        # 反向传播\n",
    "        trainer.zero_grad()  # 清除梯度\n",
    "        l.backward()  # 计算梯度\n",
    "        trainer.step()  # 更新参数\n",
    "    with torch.no_grad():\n",
    "        train_l = loss(net(features), labels)  # 在整个训练集上计算损失\n",
    "        print(f'epoch {epoch + 1}, loss {float(train_l.mean()):f}')  # 打印每个epoch的平均损失\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec7ae2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
